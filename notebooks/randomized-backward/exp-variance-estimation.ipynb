{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159466c4",
   "metadata": {},
   "source": [
    "# Variance Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc60edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.insert(0, '/workspace/few-bit-backward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a903498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7979721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58620a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from functools import partial\n",
    "from os import environ, makedirs\n",
    "from pathlib import Path\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57040aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force using of fixed CUDA device.\n",
    "if 'CUDA_VISIBLE_DEVICES' not in environ:\n",
    "    environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fde7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from torch import manual_seed\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (RobertaTokenizerFast as Tokenizer,\n",
    "                          RobertaForSequenceClassification as Model,\n",
    "                          Trainer, TrainerCallback, TrainingArguments)\n",
    "from transformers.integrations import TensorBoardCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c3fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fewbit import LinearGRP\n",
    "from fewbit.modules.variance import VarianceEstimator\n",
    "from fewbit.util import convert_linear, map_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0x12c946425095e587\n",
    "TASK = 'cola'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = Path('~/.cache/fewbit').expanduser()\n",
    "DATA_DIR = Path('../../tmp/variance/data/huggingface')\n",
    "LOG_DIR = Path('../../tmp/variance/log')\n",
    "MODEL_DIR = Path('../../tmp/variance/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6108f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_TO_KEYS = {\n",
    "    'cola': ('sentence', None),\n",
    "    'mnli': ('premise', 'hypothesis'),\n",
    "    'mnli-mm': ('premise', 'hypothesis'),\n",
    "    'mrpc': ('sentence1', 'sentence2'),\n",
    "    'qnli': ('question', 'sentence'),\n",
    "    'qqp': ('question1', 'question2'),\n",
    "    'rte': ('sentence1', 'sentence2'),\n",
    "    'sst2': ('sentence', None),\n",
    "    'stsb': ('sentence1', 'sentence2'),\n",
    "    'wnli': ('sentence1', 'sentence2'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_TO_HYPERPARAMS = {\n",
    "    'cola': (64, 1e-5),\n",
    "    'mnli': (16, 1e-5),\n",
    "    'mnli-mm': (16, 1e-5),\n",
    "    'mrpc': (16, 1e-5),\n",
    "    'qnli': (16, 1e-5),  # NOTE We use batch size 16 instead of 32.\n",
    "    'qqp': (32, 1e-5),\n",
    "    'rte': (16, 2e-5),\n",
    "    'sst2': (32, 2e-5),\n",
    "    'stsb': (16, 1e-5),\n",
    "    'wnli': (32, 1e-5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46665e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "\n",
    "parser.add_argument('-c', '--cache-dir',\n",
    "                    default=CACHE_DIR,\n",
    "                    type=Path,\n",
    "                    help='Directory to cache or original dataset files.')\n",
    "\n",
    "parser.add_argument('-d', '--data-dir',\n",
    "                    default=DATA_DIR,\n",
    "                    type=Path,\n",
    "                    help='Directory to cache preprocessed dataset files.')\n",
    "\n",
    "parser.add_argument('-l', '--log-dir',\n",
    "                    default=LOG_DIR,\n",
    "                    type=Path,\n",
    "                    help='Directory for TensorBoard logs.')\n",
    "\n",
    "parser.add_argument('-m', '--model-dir',\n",
    "                    default=MODEL_DIR,\n",
    "                    type=Path,\n",
    "                    help='Directory to save checkpoint files.')\n",
    "\n",
    "parser.add_argument('-p', '--proj-dim-ratio',\n",
    "                    default=None,\n",
    "                    type=float,\n",
    "                    help='Directory to save checkpoint files.')\n",
    "\n",
    "parser.add_argument('-s', '--seed',\n",
    "                    default=SEED,\n",
    "                    type=int,\n",
    "                    help='Random seed for reproducibility.')\n",
    "\n",
    "parser.add_argument('task',\n",
    "                    default=TASK,\n",
    "                    choices=sorted(TASK_TO_HYPERPARAMS),\n",
    "                    nargs='?',\n",
    "                    help='GLUE task to learn.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(task, metric, inputs):\n",
    "    predictions, references = inputs\n",
    "    if task != 'stsb':\n",
    "        predictions = predictions.argmax(axis=1)\n",
    "    else:\n",
    "        predictions = predictions[..., 0]\n",
    "    return metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "\n",
    "def preprocess(tokenizer, lhs, rhs, sample):\n",
    "    if rhs is None:\n",
    "        args = (sample[lhs],)\n",
    "    else:\n",
    "        args = (sample[lhs], sample[rhs])\n",
    "    return tokenizer(*args,\n",
    "                     max_length=512,\n",
    "                     padding=True,\n",
    "                     truncation=True,\n",
    "                     return_tensors='np')\n",
    "\n",
    "\n",
    "def setup(task: str,\n",
    "          cache_dir: Path = Path('cache'),\n",
    "          data_dir: Path = Path('data'),\n",
    "          model_dir: Path = Path('model'),\n",
    "          callback: Optional[TrainerCallback] = None,\n",
    "          proj_dim_ratio: Optional[float] = None):\n",
    "    # Load and configure model output head.\n",
    "    if task in ('mnli', 'mnli-mm'):\n",
    "        num_labels = 3\n",
    "    elif task == 'stsb':\n",
    "        num_labels = 1\n",
    "    else:\n",
    "        num_labels = 2\n",
    "    model_path = 'roberta-base'\n",
    "    model = Model.from_pretrained(model_path, num_labels=num_labels)\n",
    "\n",
    "    # NOTE Replace default Linear layer with ours with routine that traverses\n",
    "    # module as a tree in post-order.\n",
    "    def convert_layer(module, path):\n",
    "        return convert_linear(module, LinearGRP, proj_dim_ratio=proj_dim_ratio)\n",
    "\n",
    "    if proj_dim_ratio:\n",
    "        model = map_module(model, convert_layer)\n",
    "\n",
    "    # Load tokenizer from checkpoint.\n",
    "    tokenizer = Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # Make dataset preprocessor.\n",
    "    keys = TASK_TO_KEYS[task]\n",
    "    func = partial(preprocess, tokenizer, *keys)\n",
    "\n",
    "    # Load and preprocess dataset.\n",
    "    dataset_path = 'glue'\n",
    "    dataset_name = 'mnli' if task == 'mnli-mm' else task\n",
    "    dataset = load_dataset(dataset_path, dataset_name, cache_dir=str(data_dir))\n",
    "    dataset_cache = {key: str(cache_dir / f'glue-{task}-{key}.arrow')\n",
    "                     for key in dataset.keys()}\n",
    "    dataset_encoded = dataset.map(func,\n",
    "                                  batched=True,\n",
    "                                  cache_file_names=dataset_cache)\n",
    "\n",
    "    # Load dataset metric.\n",
    "    metric = load_metric(dataset_path, dataset_name)\n",
    "    metric_compute = partial(compute_metric, task, metric)\n",
    "\n",
    "    # Pick right evaluation metric.\n",
    "    eval_metric_name = 'accuracy'\n",
    "    if task == 'cola':\n",
    "        eval_metric_name = 'matthews_correlation'\n",
    "    elif task == 'stsb':\n",
    "        eval_metric_name = 'pearson'\n",
    "\n",
    "    # Pick right dataset for train/evaluation stage.\n",
    "    dataset_train = dataset_encoded['train']\n",
    "    dataset_eval = dataset_encoded.get('validation')\n",
    "    if task == 'mnli-mm':\n",
    "        dataset_eval = dataset_encoded['validation_mismatched']\n",
    "    elif task == 'mnli':\n",
    "        dataset_eval = dataset_encoded['validation_matched']\n",
    "\n",
    "    # Get hyperparameters from task name.\n",
    "    bs, lr = TASK_TO_HYPERPARAMS[task]\n",
    "\n",
    "    # Make 6% of total steps as warm up steps.\n",
    "    noepoches = 10\n",
    "    warmup_steps = int(0.06 * len(dataset_train) * noepoches / bs)\n",
    "\n",
    "    # Initialize training driver.\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(model_dir / f'glue-{task}'),\n",
    "        evaluation_strategy='epoch',\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs,\n",
    "        num_train_epochs=noepoches,\n",
    "        save_strategy='no',\n",
    "        #save_strategy='epoch',\n",
    "        #load_best_model_at_end=True,\n",
    "        #metric_for_best_model=eval_metric_name,\n",
    "        logging_strategy='epoch',\n",
    "        log_level='warning',\n",
    "        learning_rate=lr,\n",
    "        weight_decay=0.1,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.98,\n",
    "        adam_epsilon=1e-6,\n",
    "        lr_scheduler_type='polynomial',\n",
    "        warmup_steps=warmup_steps,\n",
    "        push_to_hub=False)\n",
    "\n",
    "    trainer = Trainer(model=model.to(DEVICE),\n",
    "                      args=args,\n",
    "                      train_dataset=dataset_train,\n",
    "                      eval_dataset=dataset_eval,\n",
    "                      tokenizer=tokenizer,\n",
    "                      compute_metrics=metric_compute,\n",
    "                     )\n",
    "#                       callbacks=[callback])\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(task: str, cache_dir: Path, data_dir: Path, log_dir: Path,\n",
    "          model_dir: Path, proj_dim_ratio: Optional[float], seed: int):\n",
    "    makedirs(cache_dir, exist_ok=True)\n",
    "    makedirs(log_dir, exist_ok=True)\n",
    "    makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    manual_seed(seed)\n",
    "\n",
    "    tensorboard_sm = SummaryWriter(log_dir / task)\n",
    "    tensorboard_cb = TensorBoardCallback(tensorboard_sm)\n",
    "\n",
    "    trainer = setup(task, cache_dir, data_dir, model_dir, tensorboard_cb,\n",
    "                    proj_dim_ratio)\n",
    "    trainer.train()\n",
    "\n",
    "    tensorboard_sm.flush()\n",
    "    tensorboard_sm.close()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check we are run by IPytton kernel.\n",
    "if getattr(builtins, '__IPYTHON__', False):\n",
    "    args = parser.parse_args(args=[])\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Run training finally!\n",
    "# train(**args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c737b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = setup('cola', CACHE_DIR, DATA_DIR, MODEL_DIR, [], 0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def log_variance(var, step, summary=None):\n",
    "    print(f'{step:d} sgd={var[0]:e} rmm={var[1]:e}')\n",
    "    if summary:\n",
    "        summary.add_scalar('variance/sgd', var[0], step)\n",
    "        summary.add_scalar('variance/rmm', var[1], step)\n",
    "        \n",
    "    \n",
    "summary = SummaryWriter('../../log/variance/test-varest')\n",
    "callback = partial(log_variance, summary=summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dea6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = map_module(root=trainer.model,\n",
    "                   func=lambda mod, _: VarianceEstimator(mod, callback),\n",
    "                   patt=r'/roberta/encoder/layer/6/intermediate/dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee02f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.roberta.encoder.layer[6].intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd4dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
